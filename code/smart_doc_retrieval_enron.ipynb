{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6bd4f0-3d46-4da1-a101-22dfd61a9fe1",
   "metadata": {},
   "source": [
    "# Smart Document Retrieval\n",
    "<p>The primary focus of the notebook is to illustrate the process of using a transformer model to embed text data into a numerical representation that can be used to calculated a similarity score as compared to a query string embedding. Additionally, we explore some of the architectural theory of a complete application.</p>\n",
    "<p>Smart Document Retrieval can be divided into the following key components:\n",
    "    <li>1) Document Management</li>\n",
    "    <li>2) Source Text Extraction</li>\n",
    "    <li>3) Source Text Storage</li>\n",
    "    <li>4) Source Text Embedding\n",
    "        <ul>\n",
    "            <li>4a) Model Selection</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>5) Source Embedding Storage and Management</li>\n",
    "    <li>6) Query String Embedding</li>\n",
    "    <li>7) Similarity Scoring and Ranking</li>\n",
    "    <li>8) Advanced techniques <a href='https://www.sbert.net/examples/applications/retrieve_rerank/README.html'>Retrieval and Re-Ranking - Bi-Encoders(Retrieval) and Cross-Encoders(Re-Ranker)</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9804333-81ef-4abd-b6d9-a2b82bcb48fd",
   "metadata": {},
   "source": [
    "## 1) Document Management\n",
    "<p>The exact manner you manage the documents/resources to use will be based on your use case and is beyond the scope of this notebook; however, it is important to consider several items.\n",
    "    <li><b>Access</b>\n",
    "        <ul>\n",
    "            <li>With the application have continuous access to source documents?</li>\n",
    "            <li>Will the application need privileged permissions?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Versioning</b>\n",
    "        <ul>\n",
    "            <li>Is there a document versioning process?</li>\n",
    "            <li>Are there duplicate/variations of a documents?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Document/Resource Types</b>\n",
    "        <ul>\n",
    "            <li>What type of document formats will be used? (e.g., MS Word, Excel, Google Docs, Websites, Emails, etc.)</li>\n",
    "            <li>Are there different format versions?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3d93e-51c3-4530-8662-8474e2c1568e",
   "metadata": {},
   "source": [
    "## 2) Source Text Extraction\n",
    "<p>The process for extracting the source text will vary by use case, we offer some things to consider during your design but there may be other considerations based on your requirements. The example dataset used in this notebook was extracted from USPTO patent XML files selecting just the abstract for embedding.\n",
    "    <li><b>Content Extraction - Technical</b>\n",
    "        <ul>\n",
    "            <li>How will you access the source text within the resource/document?</li>\n",
    "            <li>What libraries / tools will be needed to extract text?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Content Selection</b>\n",
    "        <ul>\n",
    "            <li>What parts of the document will be selected for extraction? (e.g., Subject Line, Executive Summary, Individual Sections, etc.)</li>\n",
    "            <li>If you would like the application to identify specific locations within a document that contain the relevant information you will need to extract source text at the same level.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Content Quality</b>\n",
    "        <ul>\n",
    "            <li>Do you need to remove meta-data or file formatting components such as XML tags?</li>\n",
    "            <li>Are there errors that need to be fixed? (e.g., spelling, formatting)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1556ad-8ef2-4c04-8d22-229c396760d4",
   "metadata": {},
   "source": [
    "## 3) Source Text Storage\n",
    "<p>The example dataset used in this notebook has been stored in a simple parquet file format however if your use case needs to scale to millions, billions, or more items a database may be beneficial. One option could be to use MongoDB running in its own container to store the Source Text data.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e0ca33-9f08-4a85-b7ce-1974d3ee65cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# Importing the needed libraries & Modules\n",
    "\n",
    "# Import cudf. cudf is part of the NVIDIA RAPIDS datascience SDK and is used to store the dataframes \n",
    "# used in gpu memory.\n",
    "import cudf\n",
    "\n",
    "# Import SentenceTransformer and util from the HuggingFace sentence_transformer library which has\n",
    "# been pre-installed in this environment.\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Import pickle. pickle is used to store the embedding\n",
    "import pickle\n",
    "\n",
    "# Import re for deplay\n",
    "import re\n",
    "\n",
    "# Import Path. Used to manage file system\n",
    "from pathlib import Path\n",
    "\n",
    "# Import smart_search_models. This module was created for this example to simplify the management of the \n",
    "# various models that can be used for the embedding process.\n",
    "import smart_search\n",
    "\n",
    "# Set some notebook variables\n",
    "DATASET_NAME = \"enron\"\n",
    "DATA_PATH = \"../data/\"\n",
    "MODEL_PATH = \"../models/\"\n",
    "EMBEDDING_FOLDER = DATA_PATH + \"../data/embeddings/\"\n",
    "PARQUET_PATH = DATA_PATH + '../data/enron_extracted/email_data.parquet'\n",
    "RUN_EXAMPLE_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c612c05-08b2-4fcf-99a5-789d1b0deaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists.\n"
     ]
    }
   ],
   "source": [
    "# Verify the dataset exists. If not, download, extract, and preprocess the dataset.\n",
    "file_path = Path(PARQUET_PATH)\n",
    "if file_path.exists():\n",
    "    print(\"The file exists.\")\n",
    "else:\n",
    "    print(\"The file does not exist. Setting up dataset now.\")\n",
    "    %run data_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695299cc-bb30-4a1d-9ee8-0c504706a88b",
   "metadata": {},
   "source": [
    "### Loading Example Dataset\n",
    "<p>The dataset being used in this example is comprised of over 500,000 email messages from the Enron dataset. The dataset has been downloaded and extracted using the included <a href='gather_enron_dataset.ipynb'>gather_enron_dataset</a> notebook and parsed using <a href='parse_enron_data.ipynb'>parse_enron_data</a> notebook</p>\n",
    "\n",
    "<p>The source text dataset is stored in a parquet file containing the following:\n",
    "    <li><b>file_path:</b> source file path from the Enron dataset\n",
    "    <li><b>message_id:</b> unique id assigned in the Enron dataset\n",
    "    <li><b>date:</b> email date\n",
    "    <li><b>from_address:</b> email address of the parent email\n",
    "    <li><b>to_address:</b> destination email address in the parent email\n",
    "    <li><b>org_filename:</b> name of the file source for the enron dataset\n",
    "    <li><b>is_reply_forward:</b> flag indicating if the message was a reply or forward\n",
    "    <li><b>message:</b> email message content\n",
    "    \n",
    "</p>\n",
    "<p>The method of storing the source text may vary based on your use case (e.g. CSV, parquet, JSON, MongoDB, etc..). We use a parquet file in this example for simplicity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8e6913-d9eb-4701-baf0-6ddd69ddd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 517401 entrees\n",
      "The dataset contains 234821 unique entrees\n",
      "CPU times: user 1.02 s, sys: 204 ms, total: 1.22 s\n",
      "Wall time: 1.37 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>from_address</th>\n",
       "      <th>to_address</th>\n",
       "      <th>org_filename</th>\n",
       "      <th>is_reply_forward</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/project/data/maildir/panus-s/inbox/25.</td>\n",
       "      <td>&lt;31058281.1075863216949.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Thu, 15 Nov 2001 10:35:36 -0800 (PST)</td>\n",
       "      <td>mark.greenberg@enron.com</td>\n",
       "      <td>d..gros@enron.com</td>\n",
       "      <td>SPANUS (Non-Privileged).pst</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n\\nTom -\\n\\nPlease take a look at the attache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/project/data/maildir/panus-s/inbox/28.</td>\n",
       "      <td>&lt;26834675.1075863217034.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Wed, 21 Nov 2001 17:37:47 -0800 (PST)</td>\n",
       "      <td>cheryl.johnson@enron.com</td>\n",
       "      <td>laurel.adams@enron.com, lane.alexander@enron.c...</td>\n",
       "      <td>SPANUS (Non-Privileged).pst</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n\\nAttached is the final report for November,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file_path  \\\n",
       "0  /project/data/maildir/panus-s/inbox/25.   \n",
       "1  /project/data/maildir/panus-s/inbox/28.   \n",
       "\n",
       "                                      message_id  \\\n",
       "0  <31058281.1075863216949.JavaMail.evans@thyme>   \n",
       "1  <26834675.1075863217034.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    date              from_address  \\\n",
       "0  Thu, 15 Nov 2001 10:35:36 -0800 (PST)  mark.greenberg@enron.com   \n",
       "1  Wed, 21 Nov 2001 17:37:47 -0800 (PST)  cheryl.johnson@enron.com   \n",
       "\n",
       "                                          to_address  \\\n",
       "0                                  d..gros@enron.com   \n",
       "1  laurel.adams@enron.com, lane.alexander@enron.c...   \n",
       "\n",
       "                  org_filename  is_reply_forward  \\\n",
       "0  SPANUS (Non-Privileged).pst             False   \n",
       "1  SPANUS (Non-Privileged).pst             False   \n",
       "\n",
       "                                             message  \n",
       "0  \\n\\nTom -\\n\\nPlease take a look at the attache...  \n",
       "1  \\n\\nAttached is the final report for November,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Load in the example dataset\n",
    "df = cudf.read_parquet(PARQUET_PATH).reset_index(drop=True)\n",
    "print(\"The dataset contains {} entrees\".format(df.shape[0]))\n",
    "\n",
    "# Dropping duplicates in the 'messages' column\n",
    "df = df.drop_duplicates(subset='message').reset_index(drop=True)\n",
    "print(\"The dataset contains {} unique entrees\".format(df.shape[0]))\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522c0a4-64de-45a1-a2d4-61ad96835051",
   "metadata": {},
   "source": [
    "## 4) Source Text Embedding\n",
    "<p>Historical methods for search involved simple <a href='https://en.wikipedia.org/wiki/Lexicography'>lexicographical</a> similarity pattern matching such as <a href='https://en.wikipedia.org/wiki/Regular_expression'>regex</a>. Although methods such as lexical search can be useful for some use cases, they have several disadvantages such as needing to specific the precise terms to search for. To improve search results it can be advantageous to search based on <a href='https://en.wikipedia.org/wiki/Semantic_similarity#:~:text=Semantic%20similarity%20is%20a%20metric,as%20opposed%20to%20lexicographical%20similarity.'>sematic similarity</a> using concepts rather than word for comparison.</p>\n",
    "\n",
    "<p>To be able to search by concept we must be able to represent our data in the form of concepts. This is where <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'>Transformers</a> come in. <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'>Transformers</a> are a form of Machine Learning that can be applied to Natural Language Processing (NLP), the models have been trained on extremely large datasets such as Wikipedia to develop the ability to represent input text as a highly dimensional numerical representation, this process is called <a href='https://vaclavkosar.com/ml/transformer-embeddings-and-tokenization'>embedding</a>. If this sounds complicated, don't worry the hard parts are all abstracted away for us, we just need to use the sentence transformer library. Although there are benefits of understanding how the models work, sometimes it can be just as valuable to show how easy they are to use and how impressive the results can be using off-the-shelf models. If greater accuracy is needed you can always <a href='https://www.sbert.net/docs/training/overview.html'>train transformers</a> on your own datasets to improve their capabilities.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e3398-8aa2-49c4-b71f-f4da717f990d",
   "metadata": {},
   "source": [
    "### 4a) Model Selection\n",
    "<p> There are a large number of models to choose from on <a href='https://huggingface.co/'>HuggingFace</a> even for just the task of <a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity</a>(>800 as of 11/2022). We have included a python module to help simplify organization and selection of a smaller subset of models to experiment with (~100). Using <a href='https://huggingface.co/'>HuggingFace</a> simplifies the process of downloading and running the various models, it is not the only way to consume Transformers but it was chosen as it is one of the easiest ways to get started.</p>\n",
    "    \n",
    "There are several areas to consider when selecting a model for a given task\n",
    "<li><b>Model Size</b> - Large models need more VRAM and can take longer to run but may be more 'accurate'</li>\n",
    "<li><b>Model Architecture</b> - Some models might be designed for specific use cases or finetuned for a given problem. If your use case is similar, you might have high performance out of the box.</li>\n",
    "<li><b>Task</b> - Different models have been trained for different tasks. Some examples of various tasks include; Semantic Similarity, Semantic Search, Questioning and Answering, and Document Summarization. \n",
    "    \n",
    "<p>As stated above, the models have been trained to solve a specific workflow. In our case we are trying to identify Semantically Similar documents to our query string. Within the Semantic Similarity group there are subgroups of tasks. These tasks include identifying semantically similar sentences where we try to evaluate two or more sentences and score their similarity. When the elements being evaluated are of similar length (sentence to sentence, paragraph to parapraph) the process is called <b>symmetric semantic search</b>. If you are evaluating a short query phrase or word to sentance, paragraphs, or even documents it is refered to as <b>asymmetric semantic search</b> and models have been specially trained for each type.</p>\n",
    "    \n",
    "<li><a href='https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models'>Symmetric Semantic Search Pretrained Models</a></li>\n",
    "<li><a href='https://www.sbert.net/docs/pretrained-models/msmarco-v3.html'>Asymmetric Semantic Search</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d366e-220d-4917-9395-a40fdc9dba92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the Model\n",
    "<p>Loading the model is a simple as passing the model's name as an input argument to create a model object. If the model isn't available locally it will be downloaded automatically. One of the hardest parts of working with HuggingFace is keeping track of all the models available. You can view all the models available for <a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity</a> and copy the name into the code or to simplify things we have created a very basic python module <a href='smart_search.py'>smart_search.py</a> to hold model names.</p>\n",
    "\n",
    "<details>\n",
    "  <summary>SentenceTransformer Parameters</summary>\n",
    "<li><b>model_name_or_path</b> – If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from Huggingface models repository with that name.</li>\n",
    "<li><b>modules</b> – This parameter can be used to create custom SentenceTransformer models from scratch.</li>\n",
    "<li><b>device</b> – Device (like ‘cuda’ / ‘cpu’) that should be used for computation. If None, checks if a GPU can be used.</li>\n",
    "<li><b>cache_folder</b> – Path to store models</li>\n",
    "<li><b>use_auth_token</b> – HuggingFace authentication token to download private models.</li>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d3181b-83c9-4868-80f9-e88194092c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: 'intfloat/multilingual-e5-small'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Select and load model.\n",
    "# Note: If a given model hasn't been used since the container has been loaded it will be downloaded automatically.\n",
    "\n",
    "# The sentence_models list is a large list of models. They have not been grouped by task beyond sentence similarity \n",
    "#model_name = smart_search_models.sentence_models[6]\n",
    "#model_name = smart_search_models.default_model\n",
    "\n",
    "# asymmetric_cosine_similarity_models are special purpose models for Asymmetric Semantic Similarity through cosine similarity calculations\n",
    "#model_name = smart_search.asymmetric_cosine_similarity_models[1]\n",
    "\n",
    "# symmetric_models are special purpose models for Symmetric Sematic Similarity\n",
    "#model_name = smart_search.symmetric_models[1]\n",
    "\n",
    "# Multilingual Models\n",
    "model_name = smart_search.multilingual_models[2]\n",
    "\n",
    "print(\"Loading model: '{}'\".format(model_name))\n",
    "model = SentenceTransformer(model_name,cache_folder = MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c4a8b-190e-46c2-9af3-d1b37a0f6aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Source Text Embedding\n",
    "<p>To embed the source text, we can pass the entire column of our dataset into the model object in a single line of code as shown in the cell blocks below.</p>\n",
    "\n",
    "<p>A couple important items to note here:\n",
    "    <li>You only need to embed the source text once for a given model. Depending on your use case you may wish to database the embeddings for later use, just remember to keep track of the model used for embedding and the source document.</li>\n",
    "    <li>As each model will embed the input text differently you need to ensure the source text and query text were embedded using the same model. If you choose to database or store your embedding for later just be sure to track which models were used for the embedding as you will likely get unexpected results if comparing embedding from different models.</li>\n",
    "    </p>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>encode Parameters</summary>\n",
    "    <li><b>sentences</b> – the sentences to embed</li>\n",
    "    <li><b>batch_size</b> – the batch size used for the computation</li>\n",
    "    <li><b>show_progress_bar</b> – Output a progress bar when encode sentences</li>\n",
    "    <li><b>output_value</b> – Default sentence_embedding, to get sentence embeddings. Can be set to token_embeddings to get wordpiece token embeddings. Set to None, to get all output values</li>\n",
    "    <li><b>convert_to_numpy</b> – If true, the output is a list of numpy vectors. Else, it is a list of pytorch tensors.</li>\n",
    "    <li><b>convert_to_tensor</b> – If true, you get one large tensor as return. Overwrites any setting from convert_to_numpy</li>\n",
    "    <li><b>device</b> – Which torch.device to use for the computation</li>\n",
    "    <li><b>normalize_embeddings</b> – If set to true, returned vectors will have length 1. In that case, the faster dot-product (util.dot_score) instead of cosine similarity can be used.</li>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba9e40-5d4a-4a73-a771-97c295b7875d",
   "metadata": {},
   "source": [
    "#### Performance Considerations\n",
    "The examples below show the benifits of processing an array of inputs versus iterating over a loop. Additional performance improvements can be found by utilizing Triton Inference Server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b61bd18-37b5-41a5-8c1a-128f32330dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset to be used as an example\n",
    "\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    example_size = 10000\n",
    "    example_df = df[0:example_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e3b37-71b8-42a0-b398-ee75d7f3471f",
   "metadata": {},
   "source": [
    "<p>In the cell below we call the encoder for every message. This does not take advantage parrallel processing and we can see the processing time difference. The timings are a result of test runs using an NVIDIA RTX A6000.</p>\n",
    "\n",
    "<b>NVIDIA RTX A3500 (note: Now running on CUDA 12.2)</b>\n",
    "<p>\n",
    "<li>Model: all-mpnet-base-v2</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 1min 35s --> 95 Seconds</li>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<li>Model: intfloat/multilingual-e5-large</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 4min 7s</li>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<li>Model: intfloat/multilingual-e5-small</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 1min 48s</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b83777-9526-4cfd-865b-dd9b98f8029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 3 µs, total: 8 µs\n",
      "Wall time: 11.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    # Initialize embedding list\n",
    "    source_embedding_loop = []\n",
    "    \n",
    "    # Loop though the examples and embed each message\n",
    "    for i in range(0,example_df.shape[0]):   \n",
    "        source_embedding_loop.append(model.encode(example_df['message'][i], convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a9c2c6-2a70-489d-a7dc-965f256f6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_model = SentenceTransformer(model_name,cache_folder = MODEL_PATH,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72045db3-ec6c-46d8-8d30-48fcb36192bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    # Initialize embedding list\n",
    "    source_embedding_loop = []\n",
    "    \n",
    "    # Loop though the examples and embed each message\n",
    "    for i in range(0,example_df.shape[0]):   \n",
    "        source_embedding_loop.append(cpu_model.encode(example_df['message'][i], convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3c877-8d23-46cf-bb45-5b996f15b25b",
   "metadata": {},
   "source": [
    "<p>In the cell below we call the encoder with the entire array. This method allows us to take advantage of batch processing.</p>\n",
    "\n",
    "<p>\n",
    "Model: msmarco-distilbert-base-v4\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 18.4 Seconds</li></p>\n",
    "\n",
    "<b>NVIDIA RTX A3500 (note: Now running on CUDA 12.2)</b>\n",
    "<p>\n",
    "<li>Model: intfloat/multilingual-e5-large</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 1min 28s</li>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<li>Model: intfloat/multilingual-e5-small</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 19.2s</li>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884034a5-7248-496a-80df-b24118ab1769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    source_embeddings_array = model.encode(example_df.message.to_pandas(),convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce2d5ea-c091-4cd7-a232-51fb541b3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_model = SentenceTransformer(model_name,cache_folder = MODEL_PATH,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad8956e7-2649-4b19-91bc-a67385c5a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    source_embeddings_array = cpu_model.encode(example_df.message.to_pandas(),convert_to_tensor=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf163700-952d-4122-8af0-94bd2c943af6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1815324529.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    <li>Wall Time: 7m 4s</li>\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "<p>\n",
    "CPU TIMING\n",
    "<li>Model: intfloat/multilingual-e5-small</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 7m 4s</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cc650-f624-41df-94f3-aafa8e7d4dc6",
   "metadata": {},
   "source": [
    "<p>Note that the performance was 4x, this could have a significant impact if running on the full 500,000 message.</p>\n",
    "\n",
    "Model: msmarco-distilbert-base-v4\n",
    "Corpus Size: 500,000 Messages\n",
    "Batch Size: 64\n",
    "Wall Time: 18min 5s\n",
    "\n",
    "Note: Batch size does not seem to have a significant effect on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f289d0e-c1d9-47f5-8b0c-15b8a2314eae",
   "metadata": {},
   "source": [
    "### Embedding the entire dataset\n",
    "We only need to embed the entire dataset once. We can check if the model / dataset embeddings already exist. If so just load them from disk. If not, process them. This can take as long as 30 minutes to embed the ~500,000 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d5c8345-4afa-4716-95fb-cefe3a2ae06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper functions to read and write embedding to files.\n",
    "def load_embeddings(embedding_file_path):\n",
    "        \n",
    "    #Load sentences & embeddings from disc\n",
    "    with open(embedding_file_path, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_message_id = stored_data['message_id']\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "\n",
    "    # As of now we only need the stored embeddings\n",
    "    return stored_embeddings\n",
    "\n",
    "def write_embeddings(embedding_folder, embedding_file_name,message_ids,source_embeddings):\n",
    "   \n",
    "    # Check if directory exits\n",
    "    dir_path = Path(embedding_folder)\n",
    "    \n",
    "    if not dir_path.is_dir():\n",
    "        print(\"Directory does not exist. Creating it now.\")\n",
    "        # If the directory doesn't exist create it.\n",
    "        dir_path.mkdir()\n",
    "        \n",
    "    # Create the file path\n",
    "    file_path = embedding_folder + embedding_file_name\n",
    "    \n",
    "    # Write out the embedding and message_id to disk\n",
    "    with open(file_path, \"wb\") as fOut:\n",
    "        pickle.dump({'message_id': message_ids, 'embeddings': source_embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1b71a7-1f7a-4666-b7cb-fd78f261eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file exists. Loading it now.\n",
      "../data/../data/embeddings/embeddings_enron_intfloat-multilingual-e5-small.pkl\n",
      "CPU times: user 145 ms, sys: 1.21 s, total: 1.36 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Flag for multi-gpu embedding.\n",
    "TRAIN_MULTI = False\n",
    "\n",
    "# Create the file name that would be used to store the embeddings.\n",
    "embedding_file_name = \"embeddings_{}_{}.pkl\".format(DATASET_NAME,model_name.replace('/','-'))\n",
    "\n",
    "# Create embedding Path object\n",
    "embedding_file = Path(EMBEDDING_FOLDER + embedding_file_name)\n",
    "\n",
    "# Check if the file \n",
    "if embedding_file.is_file():\n",
    "    # If a file exists with the embedding file for this dataset / model combination exists load it.\n",
    "    print(\"Embedding file exists. Loading it now.\")\n",
    "    source_embeddings = load_embeddings(embedding_file)\n",
    "else:\n",
    "    # If an embedding file does not exist. Embed the dataset and cache the data.\n",
    "    print(\"Embedding file does not exist. Creating now.\")\n",
    "    \n",
    "    if TRAIN_MULTI:\n",
    "        pool = model.start_multi_process_pool()\n",
    "        source_embeddings = model.encode_multi_process(df.message.to_pandas(),pool)\n",
    "        model.stop_multi_process_pool(pool)\n",
    "    else:\n",
    "        source_embeddings = model.encode(df.message.to_pandas(),convert_to_tensor=True,show_progress_bar=True)\n",
    "    \n",
    "    # Write out the generated embeddings\n",
    "    write_embeddings(EMBEDDING_FOLDER,embedding_file_name,df.message_id.to_pandas(),source_embeddings)\n",
    "    \n",
    "print(embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1683d3-da88-4396-a25e-4b8369b430bb",
   "metadata": {},
   "source": [
    "### Timinings\n",
    "<li>A6000 embeddings_enron_msmarco-roberta-base-v3.pkl - 34min 24s</li>\n",
    "<li>A6000 embeddings_enron_msmarco-distilbert-base-v3.pkl - 21min 55s</li>\n",
    "<li>A6000 embeddings_enron_multi-qa-mpnet-base-dot-v1 - 45min 29s</li>\n",
    "<li>A3500 embeddings_enron_msmarco-distilbert-base-v4 - 20min 3s</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b6bc1-f0a3-4c1f-946f-529bba635dcd",
   "metadata": {},
   "source": [
    "## 6) Query String Embedding\n",
    "<p>Using the same model, we then embed our query string to be used for comparison.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1c44b46-0319-4f84-8f58-d417b5684428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 0 ns, total: 1.23 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed the query string\n",
    "query_string = 'we are having a baby'\n",
    "query_embedding = model.encode(query_string,convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722045ab-8aad-4303-88ae-4663229769c5",
   "metadata": {},
   "source": [
    "## 7) Similarity Scoring and Ranking\n",
    "<p>Next, we need to calculate the similarity between the query embedding and all the source text embeddings. One of the most common approaches is to calculate the cosine similarity. Again, the complexities and math have been abstracted here with the <a href='https://www.sbert.net/docs/package_reference/util.html'>util.cos_sim</a> and sematic_search functions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d1fbabb-c332-44e6-a660-895343288bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 205 ms, sys: 0 ns, total: 205 ms\n",
      "Wall time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set k as the number of top results\n",
    "k = 100\n",
    "\n",
    "# Using the util function to run semantic search, default to cosine\n",
    "topk_results = util.semantic_search(query_embedding, source_embeddings, top_k=k)[0]\n",
    "\n",
    "# Extract the result ids\n",
    "topk_results_ids = [result['corpus_id'] for result in topk_results]\n",
    "\n",
    "# Get a dataframe of the top k results\n",
    "topk_df = df.iloc[topk_results_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eba2292-b9b3-4887-a726-36e75f8d4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're back baby.\n"
     ]
    }
   ],
   "source": [
    "# Display the message. \n",
    "\n",
    "# Using re to clean up empty lines\n",
    "msg = re.sub('\\n\\n', '', topk_df.message[0])\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a7017-3256-4da1-b08a-c011fffebda1",
   "metadata": {},
   "source": [
    "## Advanced Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb860166-738c-455e-bb44-5f029910bdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:830: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Import the cross encoder library\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "# Load the cross encoder model\n",
    "cross_model = CrossEncoder(smart_search.cross_encoder_models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a5b121a-a62b-4b87-9aae-8099ebda388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 673 ms, sys: 0 ns, total: 673 ms\n",
      "Wall time: 676 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate the cross-encoder scores and assign scores to dataframe column\n",
    "topk_df['score'] = [cross_model.predict([query_string,msg]) for msg in topk_df.message.to_pandas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5275255a-8329-4130-9c82-98a1f7023b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Many of you know that my wife, Sue, and I are expecting a baby any day now.\n",
      "When Sue stopped by the office on Tuesday, we were both pleasantly surprised\n",
      "by the wonderful gift basket and gift certificate to -- you guessed it --\n",
      "Baby's R Us that the people on the floor gave to us.  The sage advise and\n",
      "warm wishes expressed on the card were particularly appreciated.  Thank you\n",
      "so much.\n",
      "\n",
      "Tim Belden\n",
      "\n",
      "ps - it's a boy\n"
     ]
    }
   ],
   "source": [
    "# Sort the dataframe based on descending score\n",
    "topk_df = topk_df.sort_values('score',ascending=False).reset_index()\n",
    "\n",
    "# Print the top result\n",
    "print(topk_df.message[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e1a0f-ac14-45cc-babd-856acd2416ab",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "<li><a href='https://huggingface.co/'>HuggingFace</a></li>\n",
    "<li><a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity Models</a></li>\n",
    "<li><a href='https://www.sbert.net/docs/usage/semantic_textual_similarity.html'>Sematic Textual Similarity</a></li>\n",
    "\n",
    "### Environments\n",
    "#### Past Environment\n",
    "This notebook has been developed and tested on the following:\n",
    "<li>NVIDIA RTX A6000</li>\n",
    "<li>RAPIDS - rapidsai-core:22.10-cuda11.5-base-ubuntu20.04-py3.9</li>\n",
    "<li>Pytorch 1.12.1</li>\n",
    "<li>sentence-transformers</li>\n",
    "\n",
    "#### New Environment\n",
    "<li>NVIDIA RTX A3500</li>\n",
    "<li>nvcr.io/nvidia/ai-workbench/pytorch:1.0.2</li>\n",
    "<li>Pytorch 2.1</li>\n",
    "<li>sentence-transformers</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac98a6-7f9f-4efd-9996-4c88d3b706c6",
   "metadata": {},
   "source": [
    "# Additional Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e4b0a-2e9b-4110-b223-e0b887bde283",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6831288-d7eb-4961-b7fb-0dd6b76a316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "def get_top_k(query_string,source_embeddings,model, df):\n",
    "    \n",
    "    query_embedding = model.encode(query_string,convert_to_tensor=True)\n",
    "\n",
    "    k = 100\n",
    "\n",
    "    # Using the util function to run semantic search, default to cosine\n",
    "    topk_results = util.semantic_search(query_embedding, source_embeddings, top_k=k)[0]\n",
    "    \n",
    "    # Extract the result ids\n",
    "    topk_results_ids = [result['corpus_id'] for result in topk_results]\n",
    "    \n",
    "    # Get a dataframe of the top k results\n",
    "    topk_df = df.iloc[topk_results_ids].reset_index()\n",
    "\n",
    "    return topk_df\n",
    "\n",
    "def show_top_n(df,n):\n",
    "    for i in range(n):\n",
    "        # Using re to clean up empty lines\n",
    "        msg = re.sub('\\n\\n', '', df.message[i])\n",
    "        print(msg)\n",
    "\n",
    "def rerank_top_k(df):\n",
    "    # Load the cross encoder model\n",
    "    cross_model = CrossEncoder(smart_search.cross_encoder_models[1])\n",
    "\n",
    "    # Calculate the cross-encoder scores and assign scores to dataframe column\n",
    "    df['score'] = [cross_model.predict([query_string,msg]) for msg in df.message.to_pandas()]\n",
    "\n",
    "    # Sort the dataframe based on descending score\n",
    "    df = df.sort_values('score',ascending=False).reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "def run_example(query_string,source_embeddings,model, df):\n",
    "\n",
    "    topk_df = get_top_k(query_string,source_embeddings,model, df)\n",
    "\n",
    "    print(\"Showing initial results:\")\n",
    "    show_top_n(topk_df,3)\n",
    "\n",
    "    #print(\"\\nShowing reranked results:\")\n",
    "\n",
    "    #rerank_df = rerank_top_k(topk_df)\n",
    "    #show_top_n(rerank_df,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff3390-8615-46dc-a7b0-e72e59852a1e",
   "metadata": {},
   "source": [
    "## Examples in various languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a65fa6-8a05-4724-a60b-0b950892948b",
   "metadata": {},
   "source": [
    "### English: Leaving early to go fishing this weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0d8906e-3ba3-43bc-aab9-86096350bd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing initial results:\n",
      "what about football\n",
      "Team\n",
      "what game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# English\n",
    "query_string = 'Football'\n",
    "\n",
    "run_example(query_string,source_embeddings,model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02fb36-ccdb-486b-881a-fc4db5045c45",
   "metadata": {},
   "source": [
    "### French: Je pars tôt pour aller pêcher ce week-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed0e13c9-edd2-48ef-ae06-52210265d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing initial results:\n",
      "I'll have to go get one this weekend.\n",
      "i am going to denver.  i will be ready for it next week.\n",
      "i might go.  depends on this week.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# French\n",
    "query_string = 'Je pars tôt pour aller pêcher ce week-end.'\n",
    "\n",
    "run_example(query_string,source_embeddings,model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c248e5-3d21-4d8d-aeb1-849df837aeb8",
   "metadata": {},
   "source": [
    "### Russian: Ухожу рано, чтобы поехать на рыбалку в эти выходные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b61bc2ec-6225-4e6f-8c34-04aa9df2ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing initial results:\n",
      "Goin fishin' friday, so I'm leaving thurs. night\n",
      "I would like to take another day of vacation this Friday.\n",
      "Lets try before Fri, as I am planning to take that day off\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Russian\n",
    "query_string = 'Ухожу рано, чтобы поехать на рыбалку в эти выходные.'\n",
    "\n",
    "run_example(query_string,source_embeddings,model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e23da2-e568-40af-b880-2fadac31b889",
   "metadata": {},
   "source": [
    "### Chinese: 这个周末早点出发去钓鱼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "351d4f8f-831f-472e-b322-6dcd88e661ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing initial results:\n",
      "Goin fishin' friday, so I'm leaving thurs. night\n",
      "going out this weekend?  golf maybe tomorrow.\n",
      "<http://slate.msn.com/?id=2061505>\n"
     ]
    }
   ],
   "source": [
    "# Chinese (Simplified)\n",
    "query_string = '这个周末早点出发去钓鱼'\n",
    "\n",
    "run_example(query_string,source_embeddings,model, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
