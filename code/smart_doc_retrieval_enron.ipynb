{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6bd4f0-3d46-4da1-a101-22dfd61a9fe1",
   "metadata": {},
   "source": [
    "# Smart Document Retrieval\n",
    "<p>The primary focus of the notebook is to illustrate the process of using a transformer model to embed text data into a numerical representation that can be used to calculated a similarity score as compared to a query string embedding. Additionally, we explore some of the architectural theory of a complete application.</p>\n",
    "<p>Smart Document Retrieval can be divided into the following key components:\n",
    "    <li>1) Document Management</li>\n",
    "    <li>2) Source Text Extraction</li>\n",
    "    <li>3) Source Text Storage</li>\n",
    "    <li>4) Source Text Embedding\n",
    "        <ul>\n",
    "            <li>4a) Model Selection</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>5) Source Embedding Storage and Management</li>\n",
    "    <li>6) Query String Embedding</li>\n",
    "    <li>7) Similarity Scoring and Ranking</li>\n",
    "    <li>8) Advanced techniques <a href='https://www.sbert.net/examples/applications/retrieve_rerank/README.html'>Retrieval and Re-Ranking - Bi-Encoders(Retrieval) and Cross-Encoders(Re-Ranker)</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9804333-81ef-4abd-b6d9-a2b82bcb48fd",
   "metadata": {},
   "source": [
    "## 1) Document Management\n",
    "<p>The exact manner you manage the documents/resources to use will be based on your use case and is beyond the scope of this notebook; however, it is important to consider several items.\n",
    "    <li><b>Access</b>\n",
    "        <ul>\n",
    "            <li>With the application have continuous access to source documents?</li>\n",
    "            <li>Will the application need privileged permissions?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Versioning</b>\n",
    "        <ul>\n",
    "            <li>Is there a document versioning process?</li>\n",
    "            <li>Are there duplicate/variations of a documents?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Document/Resource Types</b>\n",
    "        <ul>\n",
    "            <li>What type of document formats will be used? (e.g., MS Word, Excel, Google Docs, Websites, Emails, etc.)</li>\n",
    "            <li>Are there different format versions?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3d93e-51c3-4530-8662-8474e2c1568e",
   "metadata": {},
   "source": [
    "## 2) Source Text Extraction\n",
    "<p>The process for extracting the source text will vary by use case, we offer some things to consider during your design but there may be other considerations based on your requirements. The example dataset used in this notebook was extracted from USPTO patent XML files selecting just the abstract for embedding.\n",
    "    <li><b>Content Extraction - Technical</b>\n",
    "        <ul>\n",
    "            <li>How will you access the source text within the resource/document?</li>\n",
    "            <li>What libraries / tools will be needed to extract text?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Content Selection</b>\n",
    "        <ul>\n",
    "            <li>What parts of the document will be selected for extraction? (e.g., Subject Line, Executive Summary, Individual Sections, etc.)</li>\n",
    "            <li>If you would like the application to identify specific locations within a document that contain the relevant information you will need to extract source text at the same level.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Content Quality</b>\n",
    "        <ul>\n",
    "            <li>Do you need to remove meta-data or file formatting components such as XML tags?</li>\n",
    "            <li>Are there errors that need to be fixed? (e.g., spelling, formatting)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1556ad-8ef2-4c04-8d22-229c396760d4",
   "metadata": {},
   "source": [
    "## 3) Source Text Storage\n",
    "<p>The example dataset used in this notebook has been stored in a simple parquet file format however if your use case needs to scale to millions, billions, or more items a database may be beneficial. One option could be to use MongoDB running in its own container to store the Source Text data.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e0ca33-9f08-4a85-b7ce-1974d3ee65cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# Importing the needed libraries & Modules\n",
    "\n",
    "# Import cudf. cudf is part of the NVIDIA RAPIDS datascience SDK and is used to store the dataframes \n",
    "# used in gpu memory.\n",
    "import cudf\n",
    "\n",
    "# Import SentenceTransformer and util from the HuggingFace sentence_transformer library which has\n",
    "# been pre-installed in this environment.\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Import pickle. pickle is used to store the embedding\n",
    "import pickle\n",
    "\n",
    "# Import Path. Used to manage file system\n",
    "from pathlib import Path\n",
    "\n",
    "# Import smart_search_models. This module was created for this example to simplify the management of the \n",
    "# various models that can be used for the embedding process.\n",
    "import smart_search\n",
    "\n",
    "# Set some notebook variables\n",
    "DATASET_NAME = \"enron\"\n",
    "DATA_PATH = \"../data/\"\n",
    "MODEL_PATH = \"../models/\"\n",
    "EMBEDDING_FOLDER = DATA_PATH + \"../data/embeddings/\"\n",
    "PARQUET_PATH = DATA_PATH + '../data/enron_extracted/email_data.parquet'\n",
    "RUN_EXAMPLE_DATASET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c612c05-08b2-4fcf-99a5-789d1b0deaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists.\n"
     ]
    }
   ],
   "source": [
    "# Verify the dataset exists. If not, download, extract, and preprocess the dataset.\n",
    "file_path = Path(PARQUET_PATH)\n",
    "if file_path.exists():\n",
    "    print(\"The file exists.\")\n",
    "else:\n",
    "    print(\"The file does not exist. Setting up dataset now.\")\n",
    "    %run data_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695299cc-bb30-4a1d-9ee8-0c504706a88b",
   "metadata": {},
   "source": [
    "### Loading Example Dataset\n",
    "<p>The dataset being used in this example is comprised of over 500,000 email messages from the Enron dataset. The dataset has been downloaded and extracted using the included <a href='gather_enron_dataset.ipynb'>gather_enron_dataset</a> notebook and parsed using <a href='parse_enron_data.ipynb'>parse_enron_data</a> notebook</p>\n",
    "\n",
    "<p>The source text dataset is stored in a parquet file containing the following:\n",
    "    <li><b>file_path:</b> source file path from the Enron dataset\n",
    "    <li><b>message_id:</b> unique id assigned in the Enron dataset\n",
    "    <li><b>date:</b> email date\n",
    "    <li><b>from_address:</b> email address of the parent email\n",
    "    <li><b>to_address:</b> destination email address in the parent email\n",
    "    <li><b>org_filename:</b> name of the file source for the enron dataset\n",
    "    <li><b>is_reply_forward:</b> flag indicating if the message was a reply or forward\n",
    "    <li><b>message:</b> email message content\n",
    "    \n",
    "</p>\n",
    "<p>The method of storing the source text may vary based on your use case (e.g. CSV, parquet, JSON, MongoDB, etc..). We use a parquet file in this example for simplicity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e8e6913-d9eb-4701-baf0-6ddd69ddd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 517401 entrees\n",
      "CPU times: user 3.22 s, sys: 3.39 s, total: 6.61 s\n",
      "Wall time: 6.56 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>from_address</th>\n",
       "      <th>to_address</th>\n",
       "      <th>org_filename</th>\n",
       "      <th>is_reply_forward</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/project/data/maildir/panus-s/inbox/25.</td>\n",
       "      <td>&lt;31058281.1075863216949.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Thu, 15 Nov 2001 10:35:36 -0800 (PST)</td>\n",
       "      <td>mark.greenberg@enron.com</td>\n",
       "      <td>d..gros@enron.com</td>\n",
       "      <td>SPANUS (Non-Privileged).pst</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n\\nTom -\\n\\nPlease take a look at the attache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/project/data/maildir/panus-s/inbox/28.</td>\n",
       "      <td>&lt;26834675.1075863217034.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Wed, 21 Nov 2001 17:37:47 -0800 (PST)</td>\n",
       "      <td>cheryl.johnson@enron.com</td>\n",
       "      <td>laurel.adams@enron.com, lane.alexander@enron.c...</td>\n",
       "      <td>SPANUS (Non-Privileged).pst</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n\\nAttached is the final report for November,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file_path  \\\n",
       "0  /project/data/maildir/panus-s/inbox/25.   \n",
       "1  /project/data/maildir/panus-s/inbox/28.   \n",
       "\n",
       "                                      message_id  \\\n",
       "0  <31058281.1075863216949.JavaMail.evans@thyme>   \n",
       "1  <26834675.1075863217034.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    date              from_address  \\\n",
       "0  Thu, 15 Nov 2001 10:35:36 -0800 (PST)  mark.greenberg@enron.com   \n",
       "1  Wed, 21 Nov 2001 17:37:47 -0800 (PST)  cheryl.johnson@enron.com   \n",
       "\n",
       "                                          to_address  \\\n",
       "0                                  d..gros@enron.com   \n",
       "1  laurel.adams@enron.com, lane.alexander@enron.c...   \n",
       "\n",
       "                  org_filename  is_reply_forward  \\\n",
       "0  SPANUS (Non-Privileged).pst             False   \n",
       "1  SPANUS (Non-Privileged).pst             False   \n",
       "\n",
       "                                             message  \n",
       "0  \\n\\nTom -\\n\\nPlease take a look at the attache...  \n",
       "1  \\n\\nAttached is the final report for November,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Load in the example dataset\n",
    "df = cudf.read_parquet(PARQUET_PATH).reset_index(drop=True)\n",
    "print(\"The dataset contains {} entrees\".format(df.shape[0]))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522c0a4-64de-45a1-a2d4-61ad96835051",
   "metadata": {},
   "source": [
    "## 4) Source Text Embedding\n",
    "<p>Historical methods for search involved simple <a href='https://en.wikipedia.org/wiki/Lexicography'>lexicographical</a> similarity pattern matching such as <a href='https://en.wikipedia.org/wiki/Regular_expression'>regex</a>. Although methods such as lexical search can be useful for some use cases, they have several disadvantages such as needing to specific the precise terms to search for. To improve search results it can be advantageous to search based on <a href='https://en.wikipedia.org/wiki/Semantic_similarity#:~:text=Semantic%20similarity%20is%20a%20metric,as%20opposed%20to%20lexicographical%20similarity.'>sematic similarity</a> using concepts rather than word for comparison.</p>\n",
    "\n",
    "<p>To be able to search by concept we must be able to represent our data in the form of concepts. This is where <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'>Transformers</a> come in. <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'>Transformers</a> are a form of Machine Learning that can be applied to Natural Language Processing (NLP), the models have been trained on extremely large datasets such as Wikipedia to develop the ability to represent input text as a highly dimensional numerical representation, this process is called <a href='https://vaclavkosar.com/ml/transformer-embeddings-and-tokenization'>embedding</a>. If this sounds complicated, don't worry the hard parts are all abstracted away for us, we just need to use the sentence transformer library. Although there are benefits of understanding how the models work, sometimes it can be just as valuable to show how easy they are to use and how impressive the results can be using off-the-shelf models. If greater accuracy is needed you can always <a href='https://www.sbert.net/docs/training/overview.html'>train transformers</a> on your own datasets to improve their capabilities.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e3398-8aa2-49c4-b71f-f4da717f990d",
   "metadata": {},
   "source": [
    "### 4a) Model Selection\n",
    "<p> There are a large number of models to choose from on <a href='https://huggingface.co/'>HuggingFace</a> even for just the task of <a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity</a>(>800 as of 11/2022). We have included a python module to help simplify organization and selection of a smaller subset of models to experiment with (~100). Using <a href='https://huggingface.co/'>HuggingFace</a> simplifies the process of downloading and running the various models, it is not the only way to consume Transformers but it was chosen as it is one of the easiest ways to get started.</p>\n",
    "    \n",
    "There are several areas to consider when selecting a model for a given task\n",
    "<li><b>Model Size</b> - Large models need more VRAM and can take longer to run but may be more 'accurate'</li>\n",
    "<li><b>Model Architecture</b> - Some models might be designed for specific use cases or finetuned for a given problem. If your use case is similar, you might have high performance out of the box.</li>\n",
    "<li><b>Task</b> - Different models have been trained for different tasks. Some examples of various tasks include; Semantic Similarity, Semantic Search, Questioning and Answering, and Document Summarization. \n",
    "    \n",
    "<p>As stated above, the models have been trained to solve a specific workflow. In our case we are trying to identify Semantically Similar documents to our query string. Within the Semantic Similarity group there are subgroups of tasks. These tasks include identifying semantically similar sentences where we try to evaluate two or more sentences and score their similarity. When the elements being evaluated are of similar length (sentence to sentence, paragraph to parapraph) the process is called <b>symmetric semantic search</b>. If you are evaluating a short query phrase or word to sentance, paragraphs, or even documents it is refered to as <b>asymmetric semantic search</b> and models have been specially trained for each type.</p>\n",
    "    \n",
    "<li><a href='https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models'>Symmetric Semantic Search Pretrained Models</a></li>\n",
    "<li><a href='https://www.sbert.net/docs/pretrained-models/msmarco-v3.html'>Asymmetric Semantic Search</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d366e-220d-4917-9395-a40fdc9dba92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the Model\n",
    "<p>Loading the model is a simple as passing the model's name as an input argument to create a model object. If the model isn't available locally it will be downloaded automatically. One of the hardest parts of working with HuggingFace is keeping track of all the models available. You can view all the models available for <a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity</a> and copy the name into the code or to simplify things we have created a very basic python module <a href='smart_search.py'>smart_search.py</a> to hold model names.</p>\n",
    "\n",
    "<details>\n",
    "  <summary>SentenceTransformer Parameters</summary>\n",
    "<li><b>model_name_or_path</b> – If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from Huggingface models repository with that name.</li>\n",
    "<li><b>modules</b> – This parameter can be used to create custom SentenceTransformer models from scratch.</li>\n",
    "<li><b>device</b> – Device (like ‘cuda’ / ‘cpu’) that should be used for computation. If None, checks if a GPU can be used.</li>\n",
    "<li><b>cache_folder</b> – Path to store models</li>\n",
    "<li><b>use_auth_token</b> – HuggingFace authentication token to download private models.</li>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d3181b-83c9-4868-80f9-e88194092c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: 'msmarco-roberta-base-v3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Select and load model.\n",
    "# Note: If a given model hasn't been used since the container has been loaded it will be downloaded automatically.\n",
    "\n",
    "# The sentence_models list is a large list of models. They have not been grouped by task beyond sentence similarity \n",
    "#model_name = smart_search_models.sentence_models[6]\n",
    "#model_name = smart_search_models.default_model\n",
    "\n",
    "# asymmetric_cosine_similarity_models are special purpose models for Asymmetric Semantic Similarity through cosine similarity calculations\n",
    "model_name = smart_search.asymmetric_cosine_similarity_models[1]\n",
    "\n",
    "# symmetric_models are special purpose models for Symmetric Sematic Similarity\n",
    "#model_name = smart_search.symmetric_models[1]\n",
    "\n",
    "print(\"Loading model: '{}'\".format(model_name))\n",
    "model = SentenceTransformer(model_name,cache_folder = MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c4a8b-190e-46c2-9af3-d1b37a0f6aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Source Text Embedding\n",
    "<p>To embed the source text, we can pass the entire column of our dataset into the model object in a single line of code as shown in the cell blocks below.</p>\n",
    "\n",
    "<p>A couple important items to note here:\n",
    "    <li>You only need to embed the source text once for a given model. Depending on your use case you may wish to database the embeddings for later use, just remember to keep track of the model used for embedding and the source document.</li>\n",
    "    <li>As each model will embed the input text differently you need to ensure the source text and query text were embedded using the same model. If you choose to database or store your embedding for later just be sure to track which models were used for the embedding as you will likely get unexpected results if comparing embedding from different models.</li>\n",
    "    </p>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>encode Parameters</summary>\n",
    "    <li><b>sentences</b> – the sentences to embed</li>\n",
    "    <li><b>batch_size</b> – the batch size used for the computation</li>\n",
    "    <li><b>show_progress_bar</b> – Output a progress bar when encode sentences</li>\n",
    "    <li><b>output_value</b> – Default sentence_embedding, to get sentence embeddings. Can be set to token_embeddings to get wordpiece token embeddings. Set to None, to get all output values</li>\n",
    "    <li><b>convert_to_numpy</b> – If true, the output is a list of numpy vectors. Else, it is a list of pytorch tensors.</li>\n",
    "    <li><b>convert_to_tensor</b> – If true, you get one large tensor as return. Overwrites any setting from convert_to_numpy</li>\n",
    "    <li><b>device</b> – Which torch.device to use for the computation</li>\n",
    "    <li><b>normalize_embeddings</b> – If set to true, returned vectors will have length 1. In that case, the faster dot-product (util.dot_score) instead of cosine similarity can be used.</li>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba9e40-5d4a-4a73-a771-97c295b7875d",
   "metadata": {},
   "source": [
    "#### Performance Considerations\n",
    "The examples below show the benifits of processing an array of inputs versus iterating over a loop. Additional performance improvements can be found by utilizing Triton Inference Server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b61bd18-37b5-41a5-8c1a-128f32330dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset to be used as an example\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    example_size = 10000\n",
    "    example_df = df[0:example_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e3b37-71b8-42a0-b398-ee75d7f3471f",
   "metadata": {},
   "source": [
    "<p>In the cell below we call the encoder for every message. This does not take advantage parrallel processing and we can see the processing time difference. The timings are a result of test runs using an NVIDIA RTX A6000.</p>\n",
    "<p><b>NVIDIA RTX A6000</b>\n",
    "<li>Model: all-mpnet-base-v2</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 2min 13s --> 133 Seconds</li></p>\n",
    "\n",
    "<b>NVIDIA RTX A3500 (note: Now running on CUDA 12.2)</b>\n",
    "<p>\n",
    "<li>Model: all-mpnet-base-v2</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 1min 35s --> 95 Seconds</li>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<li>Model: msmarco-roberta-base-v3</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: </li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b83777-9526-4cfd-865b-dd9b98f8029d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:371\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    368\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 371\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m truncate_embeddings(\n\u001b[1;32m    373\u001b[0m         out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate_dim\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:828\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    819\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    821\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    822\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    823\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    827\u001b[0m )\n\u001b[0;32m--> 828\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    841\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:517\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    506\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    507\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    508\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m         output_attentions,\n\u001b[1;32m    515\u001b[0m     )\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 517\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    # Initialize embedding list\n",
    "    source_embedding_loop = []\n",
    "    \n",
    "    # Loop though the examples and embed each message\n",
    "    for i in range(0,example_df.shape[0]):   \n",
    "        source_embedding_loop.append(model.encode(example_df['message'][i], convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3c877-8d23-46cf-bb45-5b996f15b25b",
   "metadata": {},
   "source": [
    "<p>In the cell below we call the encoder with the entire array. This method allows us to take advantage of batch processing. The timings are a result of test runs using an NVIDIA RTX A6000. Note we can run various batch sizes.</p>\n",
    "\n",
    "<b>NVIDIA RTX A6000</b>\n",
    "<p>Model: all-mpnet-base-v2\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 34.5 Seconds</li></p>\n",
    "\n",
    "<p>\n",
    "Model: msmarco-distilbert-base-v4\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 18.4 Seconds</li></p>\n",
    "\n",
    "<b>NVIDIA RTX A3500 (note: Now running on CUDA 12.2)</b>\n",
    "<p>\n",
    "<li>Model: msmarco-roberta-base-v3</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: </li>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884034a5-7248-496a-80df-b24118ab1769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 53s, sys: 5.97 s, total: 6min 59s\n",
      "Wall time: 6min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    source_embeddings_array = model.encode(example_df.message.to_pandas(),convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cc650-f624-41df-94f3-aafa8e7d4dc6",
   "metadata": {},
   "source": [
    "<p>Note that the performance was 4x, this could have a significant impact if running on the full 500,000 message.</p>\n",
    "\n",
    "Model: msmarco-distilbert-base-v4\n",
    "Corpus Size: 500,000 Messages\n",
    "Batch Size: 64\n",
    "Wall Time: 18min 5s\n",
    "\n",
    "Note: Batch size does not seem to have a significant effect on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f289d0e-c1d9-47f5-8b0c-15b8a2314eae",
   "metadata": {},
   "source": [
    "### Embedding the entire dataset\n",
    "We only need to embed the entire dataset once. We can check if the model / dataset embeddings already exist. If so just load them from disk. If not, process them. This can take as long as 30 minutes to embed the ~500,000 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d5c8345-4afa-4716-95fb-cefe3a2ae06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper functions to read and write embedding to files.\n",
    "def load_embeddings(embedding_file_path):\n",
    "        \n",
    "    #Load sentences & embeddings from disc\n",
    "    with open(embedding_file_path, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_message_id = stored_data['message_id']\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "\n",
    "    # As of now we only need the stored embeddings\n",
    "    return stored_embeddings\n",
    "\n",
    "def write_embeddings(embedding_folder, embedding_file_name,message_ids,source_embeddings):\n",
    "   \n",
    "    # Check if directory exits\n",
    "    dir_path = Path(embedding_folder)\n",
    "    \n",
    "    if not dir_path.is_dir():\n",
    "        print(\"Directory does not exist. Creating it now.\")\n",
    "        # If the directory doesn't exist create it.\n",
    "        dir_path.mkdir()\n",
    "        \n",
    "    # Create the file path\n",
    "    file_path = embedding_folder + embedding_file_name\n",
    "    \n",
    "    # Write out the embedding and message_id to disk\n",
    "    with open(file_path, \"wb\") as fOut:\n",
    "        pickle.dump({'message_id': message_ids, 'embeddings': source_embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b1b71a7-1f7a-4666-b7cb-fd78f261eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file does not exist. Creating now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:  19%|█▉        | 3048/16169 [26:56<1:55:58,  1.89it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:24\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:367\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    365\u001b[0m sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m    366\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(sentences_batch)\n\u001b[0;32m--> 367\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/util.py:376\u001b[0m, in \u001b[0;36mbatch_to_device\u001b[0;34m(batch, target_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[0;32m--> 376\u001b[0m         batch[key] \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Flag for multi-gpu embedding.\n",
    "TRAIN_MULTI = False\n",
    "\n",
    "# Create the file name that would be used to store the embeddings.\n",
    "embedding_file_name = \"embeddings_{}_{}.pkl\".format(DATASET_NAME,model_name)\n",
    "\n",
    "# Create embedding Path object\n",
    "embedding_file = Path(EMBEDDING_FOLDER + embedding_file_name)\n",
    "\n",
    "# Check if the file \n",
    "if embedding_file.is_file():\n",
    "    # If a file exists with the embedding file for this dataset / model combination exists load it.\n",
    "    print(\"Embedding file exists. Loading it now.\")\n",
    "    source_embeddings = load_embeddings(embedding_file)\n",
    "else:\n",
    "    # If an embedding file does not exist. Embed the dataset and cache the data.\n",
    "    print(\"Embedding file does not exist. Creating now.\")\n",
    "    \n",
    "    if TRAIN_MULTI:\n",
    "        pool = model.start_multi_process_pool()\n",
    "        source_embeddings = model.encode_multi_process(df.message.to_pandas(),pool)\n",
    "        model.stop_multi_process_pool(pool)\n",
    "    else:\n",
    "        source_embeddings = model.encode(df.message.to_pandas(),convert_to_tensor=True,show_progress_bar=True)\n",
    "    \n",
    "    # Write out the generated embeddings\n",
    "    write_embeddings(EMBEDDING_FOLDER,embedding_file_name,df.message_id.to_pandas(),source_embeddings)\n",
    "    \n",
    "print(embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1683d3-da88-4396-a25e-4b8369b430bb",
   "metadata": {},
   "source": [
    "### Timinings\n",
    "<li>A6000 embeddings_enron_msmarco-roberta-base-v3.pkl - 34min 24s</li>\n",
    "<li>A6000 embeddings_enron_msmarco-distilbert-base-v3.pkl - 21min 55s</li>\n",
    "<li>A6000 embeddings_enron_multi-qa-mpnet-base-dot-v1 - 45min 29s</li>\n",
    "<li>A3500 embeddings_enron_msmarco-distilbert-base-v4 - 20min 3s</li>\n",
    "<li>A3500 embeddingg_enron_msmarco-roberta-base-v3 - </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b6bc1-f0a3-4c1f-946f-529bba635dcd",
   "metadata": {},
   "source": [
    "## 6) Query String Embedding\n",
    "<p>Using the same model, we then embed our query string to be used for comparison.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1c44b46-0319-4f84-8f58-d417b5684428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 ms, sys: 164 µs, total: 12.7 ms\n",
      "Wall time: 11.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed the query string\n",
    "query_string = 'we are having a baby'\n",
    "query_embedding = model.encode(query_string,convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722045ab-8aad-4303-88ae-4663229769c5",
   "metadata": {},
   "source": [
    "## 7) Similarity Scoring and Ranking\n",
    "<p>Next, we need to calculate the similarity between the query embedding and all the source text embeddings. One of the most common approaches is to calculate the cosine similarity. Again, the complexities and math have been abstracted here with the <a href='https://www.sbert.net/docs/package_reference/util.html'>util.cos_sim</a> and sematic_search functions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d1fbabb-c332-44e6-a660-895343288bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 501 ms, sys: 0 ns, total: 501 ms\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set k as the number of top results\n",
    "k = 100\n",
    "\n",
    "# Using the util function to run semantic search, default to cosine\n",
    "topk_results = util.semantic_search(query_embedding, source_embeddings, top_k=k)[0]\n",
    "\n",
    "# Extract the result ids\n",
    "topk_results_ids = [result['corpus_id'] for result in topk_results]\n",
    "\n",
    "# Get a dataframe of the top k results\n",
    "topk_df = df.iloc[topk_results_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eba2292-b9b3-4887-a726-36e75f8d4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diane has passed her 5 year mark with the company, and as such is entitled to all the rights and priviledges of a veteran.  Please congratulate her with me and come admire her gift!Cara\n"
     ]
    }
   ],
   "source": [
    "# Display the message. \n",
    "import re\n",
    "\n",
    "# Using re to clean up empty lines\n",
    "msg = re.sub('\\n\\n', '', topk_df.message[0])\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a7017-3256-4da1-b08a-c011fffebda1",
   "metadata": {},
   "source": [
    "## Advanced Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb860166-738c-455e-bb44-5f029910bdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:830: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Import the cross encoder library\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "# Load the cross encoder model\n",
    "cross_model = CrossEncoder(smart_search.cross_encoder_models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a5b121a-a62b-4b87-9aae-8099ebda388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 142 ms, total: 2.11 s\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate the cross-encoder scores and assign scores to dataframe column\n",
    "topk_df['score'] = [cross_model.predict([query_string,msg]) for msg in topk_df.message.to_pandas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5275255a-8329-4130-9c82-98a1f7023b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hey,\n",
      "\n",
      "I just got an email from Linda McKula (we went to her wedding in Napa);\n",
      "anyway she was sharing the news that her and Tim are going to be having a\n",
      "baby.  She's about 2 1/2 months preggo right now.  She said things have\n",
      "gotton really quiet/slow work wise for them but they are thankful that they\n",
      "still have jobs (for now).  Later.\n",
      "\n",
      "I just found out that I might have to be in a 4 hour mtg today from 11-3; so\n",
      "let me know what your plans are this afternoon via email.\n"
     ]
    }
   ],
   "source": [
    "# Sort the dataframe based on descending score\n",
    "topk_df = topk_df.sort_values('score',ascending=False).reset_index()\n",
    "\n",
    "# Print the top result\n",
    "print(topk_df.message[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e1a0f-ac14-45cc-babd-856acd2416ab",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "<li><a href='https://huggingface.co/'>HuggingFace</a></li>\n",
    "<li><a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity Models</a></li>\n",
    "<li><a href='https://www.sbert.net/docs/usage/semantic_textual_similarity.html'>Sematic Textual Similarity</a></li>\n",
    "\n",
    "### Environments\n",
    "#### Past Environment\n",
    "This notebook has been developed and tested on the following:\n",
    "<li>NVIDIA RTX A6000</li>\n",
    "<li>RAPIDS - rapidsai-core:22.10-cuda11.5-base-ubuntu20.04-py3.9</li>\n",
    "<li>Pytorch 1.12.1</li>\n",
    "<li>sentence-transformers</li>\n",
    "\n",
    "#### New Environment\n",
    "<li>NVIDIA RTX A3500</li>\n",
    "<li>nvcr.io/nvidia/ai-workbench/pytorch:1.0.2</li>\n",
    "<li>Pytorch 2.1</li>\n",
    "<li>sentence-transformers</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da21a6-ea4b-4611-8e75-f07d33ef3a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8906e-3ba3-43bc-aab9-86096350bd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
